---
jupytext:
  formats: ipynb,md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.4
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Section 3: Advanced optimization

+++ {"editable": true, "slideshow": {"slide_type": "slide"}}

### Initial Values

To start solving any optimization problem, initial values are required.
To facilitate the definition of starting points, the `OptimizationProblem` provides a `create_initial_values` method.

+++ {"editable": true, "slideshow": {"slide_type": "fragment"}}

```{note}
This method only works if all optimization variables have defined lower and upper bounds.

Moreover, this method only guarantees that linear constraints are fulfilled.
Any nonlinear constraints may not be satisfied by the generated samples, and nonlinear parameter dependencies can be challenging to incorporate.
```

```{code-cell} ipython3
from CADETProcess.optimization import OptimizationProblem

optimization_problem = OptimizationProblem('example')

optimization_problem.add_variable('x_0', lb=0, ub=1)
optimization_problem.add_variable('x_1', lb=-0.5, ub=2.0)

optimization_problem.add_linear_constraint(
    ["x_0", "x_1"],
    lhs = [1, 0.3],
    b=1
)
```

+++ {"editable": true, "slideshow": {"slide_type": "fragment"}}

By default, the method returns a random point from the feasible region of the parameter space.
For this purpose, [hopsy](https://modsim.github.io/hopsy/) is used to efficiently (uniformly) sample the parameter space.
To create initial values, call `create_initial_values` and specify the number of individuals that should be returned.

```{code-cell} ipython3
x0 = optimization_problem.create_initial_values(10)
print(x0)
```

+++ {"slideshow": {"slide_type": "fragment"}}

Alternatively, the Chebyshev center of the polytope can be computed, which is the center of the largest Euclidean ball that is fully contained within that polytope.

```{code-cell} ipython3
x0 = optimization_problem.get_chebyshev_center()
print(x0)
```

Let's create a method to visualize these points in the parameter space.

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
def plot_initial_values(x0):
    import matplotlib.pyplot as plt
    import numpy as np
    fig, ax = plt.subplots()
    try:
        ax.scatter(x0[:, 0], x0[:, 1])
        ax.set_xlabel(r'$x_0$')
        ax.set_ylabel(r'$x_1$')
    except IndexError:
        ax.scatter(x0, np.ones_like((x0)))
        ax.set_xlabel(r'$x_0$')
    fig.tight_layout()

x0 = optimization_problem.create_initial_values(500)
plot_initial_values(x0)
```

+++ {"slideshow": {"slide_type": "slide"}}

## Parameter Normalization
Most optimization algorithms struggle when optimization variables spread over multiple orders of magnitude.
**CADET-Process** provides several transformation methods which can help to soften these challenges.

+++ {"slideshow": {"slide_type": "fragment"}}

```{figure} ./figures/transform.png
```

+++ {"slideshow": {"slide_type": "slide"}}

### Linear Normalization
The linear normalization maps the variable space from the lower and upper bound to a range between $0$ and $1$ by applying the following transformation:

$$
x^\prime = \frac{x - x_{lb}}{x_{ub} - x_{lb}}
$$

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [solution]
---
from CADETProcess.optimization import OptimizationProblem

if __name__ == "__main__":
    optimization_problem = OptimizationProblem('transform_demo')
    optimization_problem.add_variable('var_lin', lb=-100, ub=100, transform='linear')
```

+++ {"slideshow": {"slide_type": "slide"}}

### Log Normalization
The log normalization maps the variable space from the lower and upper bound to a range between $0$ and $1$ by applying the following transformation:

$$
x^\prime = \frac{log \left( \frac{x}{x_{lb}} \right) }{log \left( \frac{x_{ub} }{x_{lb}} \right) }
$$

```{code-cell} ipython3
:tags: [solution]

optimization_problem.add_variable('var_log', lb=-100, ub=100, transform='log')
```

+++ {"slideshow": {"slide_type": "slide"}}

### Auto Transform
This transform will automatically switch between a linear and a log transform if the ratio of upper and lower bounds is larger than some value ($1000$ by default).

```{code-cell} ipython3
:tags: [solution]

optimization_problem.add_variable('var_auto', lb=-100, ub=100, transform='auto')
```

```{figure} ./figures/evaluation_example.png
```

+++ {"slideshow": {"slide_type": "slide"}}

## Evaluation Objects

```{figure} ./figures/evaluation_steps.png
```

- `OptimizationVariables` usually refers to attributes of a `Process` model (e.g. model parameters / event times.
- `EvaluationObject` objects manage the value of that optimization variable
- `Evaluators` execute (intermediate) steps required for calculating the objective (e.g. simulation)

+++ {"slideshow": {"slide_type": "slide"}}

```{figure} ./figures/evaluation_single_variable.png
:width: 30%
```


To associate an `OptimizationVariable` with an `EvaluationObject`, it first needs to be added to the `OptimizationProblem`.
For this purpose, consider a simple `Process` object from the [examples collection](https://cadet-process.readthedocs.io/en/stable/examples/batch_elution/process.html).

```{code-cell} ipython3
:tags: [solution]

from examples.batch_elution.process import process

optimization_problem = OptimizationProblem('evaluator')

optimization_problem.add_evaluation_object(process)
```

+++ {"slideshow": {"slide_type": "fragment"}}

Then add the variable. In addition, specify:

- `parameter_path`: Path to the variable in the evaluation object
- `evaluation_objects`: The evaluation object(s) for which the variable should be set.

```{code-cell} ipython3
:tags: [solution]

optimization_problem.add_variable('length', evaluation_objects=[process], parameter_path='flow_sheet.column.length', lb=0, ub=1)
```

+++ {"slideshow": {"slide_type": "slide"}}

## Multiple Evaluation Objects

```{figure} ./figures/evaluation_multiple_variables.png
:width: 30%
```

```{code-cell} ipython3
:tags: [solution]

optimization_problem = OptimizationProblem('two_eval_obj')

import copy
process_2 = copy.deepcopy(process)
process_2.name = 'foo'

optimization_problem.add_evaluation_object(process)
optimization_problem.add_evaluation_object(process_2)

optimization_problem.add_variable('flow_sheet.column.length', lb=0, ub=1)
optimization_problem.add_variable('flow_sheet.column.diameter', lb=0, ub=1, evaluation_objects=process_2)
```

+++ {"slideshow": {"slide_type": "slide"}}

### Evaluators
Any callable function can be added as `Evaluator`, assuming the first argument is the result of the previous step and it returns a single result object which is then processed by the next step.

```{figure} ./figures/evaluation_steps.png
```

- Any callable function can be added as `Evaluator`.
- Each `Evaluator` takes the previous result as input and returns a new (intermediate) result.
- Intermediate results are automatically cached.

+++ {"slideshow": {"slide_type": "slide"}}

## Evaluator Example

In this example, two steps are required:
- Process Simulation
- Fractionation

```{code-cell} ipython3
---
slideshow:
  slide_type: slide
tags: [solution]
---
from CADETProcess.simulator import Cadet
simulator = Cadet()

optimization_problem.add_evaluator(simulator)

from CADETProcess.fractionation import FractionationOptimizer
frac_opt = FractionationOptimizer()

optimization_problem.add_evaluator(
    frac_opt,
    kwargs={
        'purity_required': [0.95, 0.95],
        'ignore_failed': False,
        'allow_empty_fractions': False,
    }
)
```

+++ {"slideshow": {"slide_type": "slide"}}

## Adding Objectives

Now, when adding objectives, specify which steps are required for each objective

```{code-cell} ipython3
:tags: [solution]

from CADETProcess.performance import Productivity, Recovery, Purity

productivity = Productivity()
optimization_problem.add_objective(
    productivity,
    n_objectives=2,
    requires=[simulator, frac_opt]
)

recovery = Recovery()
optimization_problem.add_objective(
    recovery,
    n_objectives=2,
    requires=[simulator, frac_opt]
)

purity = Purity()
optimization_problem.add_nonlinear_constraint(
    purity,
    n_nonlinear_constraints=2,
    requires=[simulator, frac_opt],
    bounds=[0.95, 0.95]
)
```

+++ {"slideshow": {"slide_type": "slide"}}

## Evaluate Toolchain

To check the toolchain, simply call `evaluate_objectives`

```{code-cell} ipython3
:tags: [solution]

optimization_problem.evaluate_objectives([0.5, 0.01])
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
optimization_problem.objective_labels
```

```{code-cell} ipython3

```
