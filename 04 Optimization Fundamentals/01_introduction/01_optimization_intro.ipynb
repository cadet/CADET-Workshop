{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91651af3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization\n",
    "\n",
    "One of the main applications of **CADET-Process** is performing optimization studies.\n",
    "Optimization refers to the selection of a solution with regard to some criterion.\n",
    "In the simplest case, an optimization problem consists of minimizing some function $f(x)$ by systematically varying the input values $x$ and computing the value of that function.\n",
    "\n",
    "$$\n",
    "\\min_x f(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ca6aba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the context of physico-chemical processes, examples for the application of optimization studies include scenarios such as process optimization and parameter estimation.\n",
    "Here, often many variables are subject to optimization, multiple criteria have to be balanced, and additional linear and nonlinear constraints need to be considered.\n",
    "\n",
    "$$\n",
    "\\min_x f(x) \\\\\n",
    "s.t. \\\\\n",
    "    g(x) \\le 0, \\\\\n",
    "    h(x) = 0, \\\\\n",
    "    x \\in \\mathbb{R}^n \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "where $g$ summarizes all inequality constraint functions, and $h$ equality constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5ce67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the following, the `optimization` module of **CADET-Process** is introduced. To decouple the problem formulation from the problem solution, two classes are provided:\n",
    "- An `OptimizationProblem` class to specify optimization variables, objectives and constraints, and\n",
    "- an abstract `Optimizer` interface which allows using different external optimizers to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0d9dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 1: Minimization of a quadratic function\n",
    "\n",
    "Usually, the objective function is not known; it can only be evaluated at certain points.\n",
    "For demonstration purpouses, consider a quadratic function to be minimized.\n",
    "\n",
    "$$\n",
    "f(x) = x^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b3d7b",
   "metadata": {},
   "source": [
    "Since we already know a lot about this function, it can help to introduce some of the Optimization concepts of CADET-Process.\n",
    "For example, the results should yield:\n",
    "- $x_{opt} = 0$\n",
    "- $f_{opt} = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584291a4",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 101)\n",
    "y = x**2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7844e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OptimizationProblem\n",
    "\n",
    "The `OptimizationProblem` class is is used to specify optimization variables, objectives and constraints.\n",
    "After import, the `OptimizationProblem` initialized with a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae10e88",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from CADETProcess.optimization import OptimizationProblem\n",
    "\n",
    "optimization_problem = OptimizationProblem('quadratic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c8c75",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Optimization Variables\n",
    "Any number of variables can be added to the `OptimizationProblem`.\n",
    "To add a variable, use the `add_variable` method.\n",
    "In this case, there is only a single variable.\n",
    "The first argument is the name of the variable.\n",
    "Moreover, lower and upper bounds can be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e3ed8",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "optimization_problem.add_variable(name='x', lb=-10, ub=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f1a5d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The total number of variables is stored in `n_variables` and the names in `variable_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f590f9",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "print(optimization_problem.n_variables)\n",
    "print(optimization_problem.variable_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20bf40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objectives\n",
    "Any `callable` (i.e. an object that can be called using the `( )` operator) can be added as an objective as long as it takes x as the first argument.\n",
    "Multi-objective optimization is also possible with CADET-Python (more on that later).\n",
    "For now, the objective must return a single, scalar value.\n",
    "\n",
    "```{note}\n",
    "Usually, there are multiple variables involved. Hence, the function is expected to accept a list.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a67b6b",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def objective_fun(x):\n",
    "    return x[0] ** 2\n",
    "\n",
    "optimization_problem.add_objective(objective_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f801f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To evaluate the this function, the `evaluate_objective` method can be used.\n",
    "This is useful to test whether everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378c46d1",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "print(optimization_problem.evaluate_objectives([2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad37a41",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If the value is out of bounds, an error will be thrown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e823ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizer\n",
    "The `OptimizerAdapter` provides a unified interface for using external optimization libraries.\n",
    "It is responsible for converting the `OptimizationProblem` to the specific API of the external `Optimizer`.\n",
    "Currently, adapters to **Pymoo** and **Scipy's** optimization suite are implemented, all of which are published under open source licenses that allow for academic as well as commercial use.\n",
    "\n",
    "Before the optimization can be run, the `Optimizer` needs to be initialized and configured.\n",
    "For this example, `U_NSGA3` is used, a genetic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7325f46",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from CADETProcess.optimization import U_NSGA3\n",
    "\n",
    "optimizer = U_NSGA3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ae3d5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "All options can be displayed the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff44e68",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "print(optimizer.options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6d114",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To reduce the calculation time, let's limit the maximum number of generations that the `Optimizer` evaluates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd052dbc",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "optimizer.n_max_gen = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226f55e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To optimize the `OptimizationProblem`, call the `optimize()` method.\n",
    "By default, CADET-Process tries to autogenerate initial values.\n",
    "However, it's also possible to pass them as an additional `x0` argument.\n",
    "More on generating initial values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb9ef1",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "optimization_results = optimizer.optimize(optimization_problem, x0=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782bd9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization Progress and Results\n",
    "\n",
    "The `OptimizationResults` which are returned contain information about the progress of the optimization.\n",
    "For example, the attributes `x` and `f` contain the final value(s) of parameters and the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d9ab4",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "print(optimization_results.x)\n",
    "print(optimization_results.f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f64e6f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After optimization, several figures can be plotted to vizualize the results.\n",
    "For example, the convergence plot shows how the function value changes with the number of evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273202ed",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "optimization_results.plot_convergence('objectives')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c5808",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `plot_objectives` method shows the objective function values of all evaluated individuals.\n",
    "Here, lighter color represent later evaluations.\n",
    "Note that by default the values are plotted on a log scale if they span many orders of magnitude.\n",
    "To disable this, set `autoscale=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293dcc8c",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "optimization_results.plot_objectives(autoscale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5536357",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that more figures are created for constrained optimization, as well as multi-objective optimization.\n",
    "All figures are also saved automatically in the `working_directory`.\n",
    "Moreover, results are stored in a `.csv` file.\n",
    "- The `results_all.csv` file contains information about all evaluated individuals.\n",
    "- The `results_last.csv` file contains information about the last generation of evaluated individuals.\n",
    "- The `results_pareto.csv` file contains only the best individual(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95db40",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{subject to: } \\\\\n",
    "x_0 + 2 x_1 \\leq 1 \\\\\n",
    "x_0^2 + x_1 \\leq 1 \\\\\n",
    "                    x_0^2 - x_1 \\leq 1 \\\\\n",
    "                    2 x_0 + x_1 = 1 \\\\\n",
    "                    0 \\leq x_0 \\leq 1 \\\\\n",
    "                    -0.5 \\leq x_1 \\leq 2.0.\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1379af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2: Constrained Optimization\n",
    "\n",
    "Example taken from [SciPy Documentation](https://docs.scipy.org/doc/scipy/tutorial/optimize.html#id34)\n",
    "\n",
    "As an example let us consider the constrained minimization of the Rosenbrock function:\n",
    "\n",
    "$$\n",
    "\\min_{x_0, x_1} & ~~100\\left(x_{1}-x_{0}^{2}\\right)^{2}+\\left(1-x_{0}\\right)^{2} &\\\\\n",
    "    \\text{subject to: } & x_0 + 2 x_1 \\leq 1 & \\\\\n",
    "                        & x_0^2 + x_1 \\leq 1  & \\\\\n",
    "                        & x_0^2 - x_1 \\leq 1  & \\\\\n",
    "                        & 2 x_0 + x_1 = 1 & \\\\\n",
    "                        & 0 \\leq  x_0  \\leq 1 & \\\\\n",
    "                        & -0.5 \\leq  x_1  \\leq 2.0. &\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369b001",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This optimization problem has the unique solution $[x_0, x_1] = [0.4149,~ 0.1701]$.\n",
    "\n",
    "```{figure} ./figures/rosenbrock.png\n",
    ":align: center\n",
    ":width: 50%\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71d1702",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To setup this problem, first a new `OptimizationProblem` is created and the variables are added, including bounds.\n",
    "It is important to note, that `x0` cannot be used as variable name since it is reserved for the initial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ab49d",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from CADETProcess.optimization import OptimizationProblem\n",
    "\n",
    "rosenbrock_problem = OptimizationProblem('rosenbrock')\n",
    "\n",
    "rosenbrock_problem.add_variable('x_0', lb=0, ub=1)\n",
    "rosenbrock_problem.add_variable('x_1', lb=-0.5, ub=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d7b61",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, then objective function is defined and added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_objective(x):\n",
    "    x_0 = x[0]\n",
    "    x_1 = x[1]\n",
    "\n",
    "    return 100 * (x_1 - x_0 ** 2) ** 2 + (1 - x_0) ** 2\n",
    "\n",
    "\n",
    "rosenbrock_problem.add_objective(rosenbrock_objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51444184",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear inequality constraints\n",
    "Linear constraints are usually defined in the following way\n",
    "\n",
    "$$\n",
    "A \\cdot x \\leq b\n",
    "$$\n",
    "\n",
    "In **CADET-Process**, add each row $a$ of the constraint matrix needs to be added individually.\n",
    "The `add_linear_constraint` function takes the variables subject to the constraint as first argument.\n",
    "The left-hand side $a$ and the bound $b_a$ are passed as second and third argument.\n",
    "It is important to note that the column order in $a$ is inferred from the order in which the optimization variables are passed.\n",
    "\n",
    "To add the constraints of the Rosenbrock function to the optimization problem, add the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fe6dbd",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "rosenbrock_problem.add_linear_constraint(['x_0', 'x_1'], [1, 2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c5eaf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To wheck if a point fulfils the linear inequality constraints, use the `check_linear_constraints` method.\n",
    "It returns `True` if the point is within bounds and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c529b",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "rosenbrock_x0 = [0.5, 0]\n",
    "rosenbrock_problem.check_linear_constraints(rosenbrock_x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230e8b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear equality constraints\n",
    "Linear equality constraints are usually defined in the following way\n",
    "\n",
    "$$\n",
    "A_{eq} \\cdot x = b_{eq}\n",
    "$$\n",
    "\n",
    "In **CADET-Process**, add each row $a_{eq}$ of the constraint matrix needs to be added individually.\n",
    "The `add_linear_equality_constraint` function takes the variables subject to the constraint as first argument.\n",
    "The left-hand side $a_{eq}$ and the bound $b_{eq, a}$ are passed as second and third argument.\n",
    "It is important to note that the column order in $a$ is inferred from the order in which the optimization variables are passed.\n",
    "\n",
    "To add this constraint of the Rosenbrock function\n",
    "\n",
    "$$\n",
    "2 x_0 + x_1 = 1\n",
    "$$\n",
    "\n",
    "to the optimization problem, add the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea97430",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "rosenbrock_problem.add_linear_equality_constraint(['x_0', 'x_1'], lhs=[2, 1], beq=1, eps=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3e5084",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To wheck if a point fulfils the linear equality constraints, use the `check_linear_equality_constraints` method.\n",
    "It returns `True` if the point is within bounds and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae0dbd",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "rosenbrock_problem.check_linear_equality_constraints(rosenbrock_x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f51177",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nonlinear constraints\n",
    "It is also possible to add nonlinear constraints to the `OptimizationProblem`.\n",
    "\n",
    "$$\n",
    "g(x) \\le 0 \\\\\n",
    "$$\n",
    "\n",
    "In contrast to linear constraints, and analogous to objective functions, nonlinear constraints need to be added as a callable functions.\n",
    "Note that multiple nonlinear constraints can be added.\n",
    "In addition to the function, lower or upper bounds can be added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819116d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To add the constraints of the Rosenbrock function to the optimization problem, add the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70c194",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def nonlincon_1(x):\n",
    "    x_0 = x[0]\n",
    "    x_1 = x[1]\n",
    "\n",
    "    return x_0 ** 2 + x_1\n",
    "\n",
    "\n",
    "rosenbrock_problem.add_nonlinear_constraint(nonlincon_1, bounds=[1])\n",
    "\n",
    "\n",
    "def nonlincon_2(x):\n",
    "    x_0 = x[0]\n",
    "    x_1 = x[1]\n",
    "\n",
    "    return x_0 ** 2 - x_1\n",
    "\n",
    "\n",
    "rosenbrock_problem.add_nonlinear_constraint(nonlincon_2, bounds=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095345a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Again, the function can be evaluated manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790602f",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "rosenbrock_problem.check_nonlinear_constraints(rosenbrock_x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06026a1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizer\n",
    "To solve this problem, a **trust region method** is used, here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289cdb58",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from CADETProcess.optimization import TrustConstr\n",
    "\n",
    "trust_const_optimizer = TrustConstr()\n",
    "\n",
    "rosenbrock_optimization_results = trust_const_optimizer.optimize(\n",
    "    rosenbrock_problem,\n",
    "    x0=rosenbrock_x0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c44a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization Progress and Results\n",
    "\n",
    "Since in this problem, nonlinear constraints are involved, their convergence can also be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8415669",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "rosenbrock_optimization_results.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286df12",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "rosenbrock_optimization_results.plot_convergence('objectives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0476783",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "rosenbrock_optimization_results.plot_convergence('nonlinear_constraints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f0b8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "rosenbrock_optimization_results.plot_objectives()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731e82a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Initial Values\n",
    "\n",
    "To start solving any optimization problem, initial values are required.\n",
    "To facilitate the definition of starting points, the `OptimizationProblem` provides a `create_initial_values` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036aebc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{note}\n",
    "This method only works if all optimization variables have defined lower and upper bounds.\n",
    "\n",
    "Moreover, this method only guarantees that linear constraints are fulfilled.\n",
    "Any nonlinear constraints may not be satisfied by the generated samples, and nonlinear parameter dependencies can be challenging to incorporate.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fec93a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By default, the method returns a random point from the feasible region of the parameter space.\n",
    "For this purpose, [hopsy](https://modsim.github.io/hopsy/) is used to efficiently (uniformly) sample the parameter space.\n",
    "To create initial values, call `create_initial_values` and specify the number of individuals that should be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = rosenbrock_problem.create_initial_values(10)\n",
    "print(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeacea2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively, the Chebyshev center of the polytope can be computed, which is the center of the largest Euclidean ball that is fully contained within that polytope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daef8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = rosenbrock_problem.get_chebyshev_center()\n",
    "print(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e453683",
   "metadata": {},
   "source": [
    "Let's create a method to visualize these points in the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f9a6f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_initial_values(x0):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    fig, ax = plt.subplots()\n",
    "    try:\n",
    "        ax.scatter(x0[:, 0], x0[:, 1])\n",
    "        ax.set_xlabel(r'$x_0$')\n",
    "        ax.set_ylabel(r'$x_1$')\n",
    "    except IndexError:\n",
    "        ax.scatter(x0, np.ones_like((x0)))\n",
    "        ax.set_xlabel(r'$x_0$')\n",
    "    fig.tight_layout()\n",
    "\n",
    "x0 = rosenbrock_problem.create_initial_values(500)\n",
    "plot_initial_values(x0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
